// main.ts — Kemet language lexer.
//
// repo   : https://github.com/kemet-dev/lexer
// docs   : https://kemet-dev.github.io/lexer
// author : https://github.com/maysara-elshewehy
//
// Developed with ❤️ by Maysara.



// ╔════════════════════════════════════════ PACK ════════════════════════════════════════╗

    import { compile, Lexer, error as _Error} from 'moo';

// ╚══════════════════════════════════════════════════════════════════════════════════════╝



// ╔════════════════════════════════════════ INIT ════════════════════════════════════════╗

    // The `Token` interface defines the structure of a token object.
    // Each token has a type, value, and position in the source code.
    export interface Token {
		type		    : string;
		value		    : string | null;
		pos 		    : { line : number; col : number; };
	}

    // The `Rules` type is an alias for the `Lexer` type from the `moo` library.
    // It represents the lexer rules used to tokenize the source code.
    export type Rules = Lexer;

    // The `error` constant is an alias for the `error` function from the `moo` library.
    // It is used to create an error token in the lexer.
    export const error = _Error;

// ╚══════════════════════════════════════════════════════════════════════════════════════╝



// ╔════════════════════════════════════════ CORE ════════════════════════════════════════╗

    // The `createRules` constant is an alias for the `compile` function from the `moo` library.
    // It is used to create the lexer rules used to tokenize the source code.
    export const createRules = compile;

    // The `tokenize` function takes a set of lexer rules and a source code string as input.
    // It returns an array of tokens generated by the lexer.
    export const tokenize = function (rules: Rules, source_code: string) {
        // Initialize an array to hold the tokens and a position counter
		const tokens    : Token[] = [];
        let position    : number  = 0;

        // Apply the lexer rules to the input string
        rules.reset(source_code);

        // Iterate through the tokens generated by the lexer
		for (const token of rules) {

            // Push the token with its type, value, and position
			tokens.push({
                type    : token.type!,
                value   : token.value.length ? token.value : null,
                pos     : { line: token.line, col: token.col }
            });

            // If the token is an error, we stop processing further tokens
            if(token.type === "error") break;

            // Advance position
            ++position;
		}

        // Return the array of tokens
		return tokens;
    };

    // default export
    export default { createRules, error, tokenize };

// ╚══════════════════════════════════════════════════════════════════════════════════════╝