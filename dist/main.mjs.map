{"version":3,"sources":["../src/main.ts"],"sourcesContent":["// main.ts — Kemet language lexer.\n//\n// repo   : https://github.com/kemet-dev/lexer\n// docs   : https://kemet-dev.github.io/lexer\n// author : https://github.com/maysara-elshewehy\n//\n// Developed with ❤️ by Maysara.\n\n\n\n// ╔════════════════════════════════════════ PACK ════════════════════════════════════════╗\n\n    import { compile, Lexer, error as _Error} from 'moo';\n    export const createRules = compile;\n\n// ╚══════════════════════════════════════════════════════════════════════════════════════╝\n\n\n\n// ╔════════════════════════════════════════ INIT ════════════════════════════════════════╗\n\n    // The `Token` interface defines the structure of a token object.\n    // Each token has a type, value, and position in the source code.\n    export interface Token {\n\t\ttype\t\t    : string;\n\t\tvalue\t\t    : string | null;\n\t\tpos \t\t    : { line : number; col : number; };\n\t}\n\n    // The `Rules` type is an alias for the `Lexer` type from the `moo` library.\n    // It represents the lexer rules used to tokenize the source code.\n    export type Rules = Lexer;\n\n    // The `error` constant is an alias for the `error` function from the `moo` library.\n    // It is used to create an error token in the lexer.\n    export const error = _Error;\n\n// ╚══════════════════════════════════════════════════════════════════════════════════════╝\n\n\n\n// ╔════════════════════════════════════════ CORE ════════════════════════════════════════╗\n\n    /**\n     * Tokenizes the given source code string according to the lexer rules.\n     *\n     * @param rules The lexer rules to apply to the source code.\n     * @param source_code The source code to tokenize.\n     * @returns An array of tokens. If a token is an error, it will be the last token in the array.\n    */\n    export const tokenize = function (rules: Rules, source_code: string) {\n        // Initialize an array to hold the tokens and a position counter\n\t\tconst tokens    : Token[] = [];\n        let position    : number  = 0;\n\n        // Apply the lexer rules to the input string\n        rules.reset(source_code);\n\n        // Iterate through the tokens generated by the lexer\n\t\tfor (const token of rules) {\n\n            // Push the token with its type, value, and position\n\t\t\ttokens.push({\n                type    : token.type!,\n                value   : token.value.length ? token.value : null,\n                pos     : { line: token.line, col: token.col }\n            });\n\n            // If the token is an error, we stop processing further tokens\n            if(token.type === \"error\") break;\n\n            // Advance position\n            ++position;\n\t\t}\n\n        // Return the array of tokens\n\t\treturn tokens;\n    };\n\n// ╚══════════════════════════════════════════════════════════════════════════════════════╝"],"mappings":";AAYI,SAAS,SAAgB,SAAS,cAAa;AACxC,IAAM,cAAc;AAsBpB,IAAM,QAAQ;AAed,IAAM,WAAW,SAAU,OAAc,aAAqB;AAEvE,QAAM,SAAsB,CAAC;AACvB,MAAI,WAAwB;AAG5B,QAAM,MAAM,WAAW;AAG7B,aAAW,SAAS,OAAO;AAG1B,WAAO,KAAK;AAAA,MACC,MAAU,MAAM;AAAA,MAChB,OAAU,MAAM,MAAM,SAAS,MAAM,QAAQ;AAAA,MAC7C,KAAU,EAAE,MAAM,MAAM,MAAM,KAAK,MAAM,IAAI;AAAA,IACjD,CAAC;AAGD,QAAG,MAAM,SAAS,QAAS;AAG3B,MAAE;AAAA,EACZ;AAGA,SAAO;AACL;","names":[]}